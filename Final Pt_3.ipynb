{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some initial comments: Even as an EDA assignment, this is a work in progress. I've played around quite a bit with the data, and I think some of that is a reflection of my level of comfort (or lack thereof) with some modeling processes. Still, I hope this constitutes a reasonable beginning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bigfile.csv',\n",
       " 'countmerge.csv',\n",
       " 'EventData.csv',\n",
       " 'gatewayEvents.csv',\n",
       " 'gw2.csv',\n",
       " 'gwold.csv',\n",
       " 'LoginData.csv',\n",
       " 'PartnerData.csv',\n",
       " 'pwresets.csv',\n",
       " 'pw_data.csv',\n",
       " 'Salesforce.csv',\n",
       " 'saveme.csv',\n",
       " 'zips.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import datetime as DT\n",
    "import collections, os\n",
    "\n",
    "\n",
    "os.chdir('C:\\Users\\jhoukal\\Desktop\\New folder')\n",
    "\n",
    "files = [f for f in os.listdir(os.getcwd()) if f.endswith('csv')]\n",
    "files #I'd like to make this part dynamic, but for the meantime I at least get a quick list of the files I want to upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Login = pd.read_csv('LoginData.csv')\n",
    "Login.rename(columns={ Login.columns[0]: \"ENTITY\"}, inplace=True)\n",
    "\n",
    "Partner = pd.read_csv('PartnerData.csv')\n",
    "Partner.rename(columns={ Partner.columns[0]: \"ENTITY\"}, inplace=True)\n",
    "\n",
    "Events = pd.read_csv('EventData.csv')\n",
    "Events.rename(columns={ Events.columns[0]: \"ENTITY\"}, inplace=True)\n",
    "\n",
    "SF = pd.read_csv('Salesforce.csv')\n",
    "SF.rename(columns={ SF.columns[0]: \"ENTITY\"}, inplace=True)\n",
    "SF.drop_duplicates(['ENTITY'], inplace=True) #because multiple case records are logged for each user, thereby duplicating users\n",
    "\n",
    "Zips = pd.read_csv('Zips.csv') #zip code data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     7/22/1971\n",
       "1     7/22/1971\n",
       "2    11/25/1977\n",
       "3      6/1/1958\n",
       "4     8/23/1974\n",
       "Name: DATE_OF_BIRTH, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Login.loc[:, 'DATE_OF_BIRTH'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "now=pd.Timestamp(DT.datetime.now()) #for age calculation\n",
    "Login.loc[:, 'DATE_OF_BIRTH'] = pd.to_datetime(Login.loc[:, 'DATE_OF_BIRTH'])\n",
    "Login.loc[:, 'AGE'] = (now - Login.loc[:, 'DATE_OF_BIRTH']).astype('<m8[Y]')\n",
    "Login.head()\n",
    "Login.drop(['DATE_OF_BIRTH'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(Login, \n",
    "              pd.merge(Events, \n",
    "                       Partner, \n",
    "                       on='ENTITY', how='outer'),\n",
    "                  on='ENTITY', how='outer')\n",
    "\n",
    "df.drop_duplicates(['ENTITY'], inplace=True) #occasional duplicate users here, too. I don't need them. \n",
    "\n",
    "df['CONTACT'] = np.where(df['ENTITY'].isin(SF['ENTITY']), True,np.nan) #true/false if member actually contacted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "df.set_index('ENTITY', inplace=True) #let's set our index, too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIRST_APP_LOGIN_DATE    276182\n",
      "FIRST_WEB_LOGIN_DATE     20112\n",
      "earned_status_adj        20157\n",
      "hra_completion_date     229520\n",
      "vc_completion_date      381478\n",
      "COVERAGE_EFF_DATE        20047\n",
      "ZIP_CD                   44178\n",
      "AGE                      20047\n",
      "eventdesc               498705\n",
      "partner_name            288187\n",
      "CONTACT                 457902\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum()) #checking range of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIRST_APP_LOGIN_DATE</th>\n",
       "      <th>FIRST_WEB_LOGIN_DATE</th>\n",
       "      <th>earned_status_adj</th>\n",
       "      <th>hra_completion_date</th>\n",
       "      <th>vc_completion_date</th>\n",
       "      <th>COVERAGE_EFF_DATE</th>\n",
       "      <th>ZIP_CD</th>\n",
       "      <th>AGE</th>\n",
       "      <th>eventdesc</th>\n",
       "      <th>partner_name</th>\n",
       "      <th>CONTACT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENTITY</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.400067e+09</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4/8/2015</td>\n",
       "      <td>Bronze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/23/2015</td>\n",
       "      <td>77064</td>\n",
       "      <td>47.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.400078e+09</th>\n",
       "      <td>NaN</td>\n",
       "      <td>11/21/2016</td>\n",
       "      <td>Bronze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1/1/2014</td>\n",
       "      <td>97405</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.400081e+09</th>\n",
       "      <td>4/3/2017</td>\n",
       "      <td>2/8/2013</td>\n",
       "      <td>Platinum</td>\n",
       "      <td>1/1/2018 0:00</td>\n",
       "      <td>3/9/2018 0:00</td>\n",
       "      <td>1/1/2013</td>\n",
       "      <td>32832</td>\n",
       "      <td>60.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>APPLE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.400100e+09</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1/11/2013</td>\n",
       "      <td>Silver</td>\n",
       "      <td>1/29/2018 0:00</td>\n",
       "      <td>2/13/2018 0:00</td>\n",
       "      <td>1/1/2011</td>\n",
       "      <td>55442</td>\n",
       "      <td>44.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.400104e+09</th>\n",
       "      <td>4/12/2016</td>\n",
       "      <td>4/12/2016</td>\n",
       "      <td>Platinum</td>\n",
       "      <td>12/1/2017 0:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/3/2014</td>\n",
       "      <td>61115</td>\n",
       "      <td>53.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Google Fit</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             FIRST_APP_LOGIN_DATE FIRST_WEB_LOGIN_DATE earned_status_adj  \\\n",
       "ENTITY                                                                     \n",
       "1.400067e+09                  NaN             4/8/2015            Bronze   \n",
       "1.400078e+09                  NaN           11/21/2016            Bronze   \n",
       "1.400081e+09             4/3/2017             2/8/2013          Platinum   \n",
       "1.400100e+09                  NaN            1/11/2013            Silver   \n",
       "1.400104e+09            4/12/2016            4/12/2016          Platinum   \n",
       "\n",
       "             hra_completion_date vc_completion_date COVERAGE_EFF_DATE ZIP_CD  \\\n",
       "ENTITY                                                                         \n",
       "1.400067e+09                 NaN                NaN         2/23/2015  77064   \n",
       "1.400078e+09                 NaN                NaN          1/1/2014  97405   \n",
       "1.400081e+09       1/1/2018 0:00      3/9/2018 0:00          1/1/2013  32832   \n",
       "1.400100e+09      1/29/2018 0:00     2/13/2018 0:00          1/1/2011  55442   \n",
       "1.400104e+09      12/1/2017 0:00                NaN         11/3/2014  61115   \n",
       "\n",
       "               AGE eventdesc partner_name  CONTACT  \n",
       "ENTITY                                              \n",
       "1.400067e+09  47.0       NaN          NaN      NaN  \n",
       "1.400078e+09  40.0       NaN          NaN      1.0  \n",
       "1.400081e+09  60.0       NaN        APPLE      NaN  \n",
       "1.400100e+09  44.0       NaN          NaN      1.0  \n",
       "1.400104e+09  53.0       NaN   Google Fit      1.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ENTITY\n",
       "1.418832e+09    NaN\n",
       "1.422994e+09    NaN\n",
       "1.424272e+09    NaN\n",
       "1.406092e+09    NaN\n",
       "1.424398e+09    NaN\n",
       "Name: AGE, dtype: category\n",
       "Categories (5, object): [0-18 < 19-34 < 35-49 < 50-64 < 65-80]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#need to bin ages\n",
    "\n",
    "df['AGE'] = pd.cut(df['AGE'], [0, 18, 34, 49, 64, 80], labels=['0-18', '19-34', '35-49', '50-64', '65-80'])\n",
    "\n",
    "df['AGE'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIRST_APP_LOGIN_DATE</th>\n",
       "      <th>FIRST_WEB_LOGIN_DATE</th>\n",
       "      <th>earned_status_adj</th>\n",
       "      <th>hra_completion_date</th>\n",
       "      <th>vc_completion_date</th>\n",
       "      <th>COVERAGE_EFF_DATE</th>\n",
       "      <th>ZIP_CD</th>\n",
       "      <th>eventdesc</th>\n",
       "      <th>partner_name</th>\n",
       "      <th>CONTACT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENTITY</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.400067e+09</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4/8/2015</td>\n",
       "      <td>Bronze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/23/2015</td>\n",
       "      <td>77064</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.400078e+09</th>\n",
       "      <td>NaN</td>\n",
       "      <td>11/21/2016</td>\n",
       "      <td>Bronze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1/1/2014</td>\n",
       "      <td>97405</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.400081e+09</th>\n",
       "      <td>4/3/2017</td>\n",
       "      <td>2/8/2013</td>\n",
       "      <td>Platinum</td>\n",
       "      <td>1/1/2018 0:00</td>\n",
       "      <td>3/9/2018 0:00</td>\n",
       "      <td>1/1/2013</td>\n",
       "      <td>32832</td>\n",
       "      <td>NaN</td>\n",
       "      <td>APPLE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.400100e+09</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1/11/2013</td>\n",
       "      <td>Silver</td>\n",
       "      <td>1/29/2018 0:00</td>\n",
       "      <td>2/13/2018 0:00</td>\n",
       "      <td>1/1/2011</td>\n",
       "      <td>55442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.400104e+09</th>\n",
       "      <td>4/12/2016</td>\n",
       "      <td>4/12/2016</td>\n",
       "      <td>Platinum</td>\n",
       "      <td>12/1/2017 0:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/3/2014</td>\n",
       "      <td>61115</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Google Fit</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             FIRST_APP_LOGIN_DATE FIRST_WEB_LOGIN_DATE earned_status_adj  \\\n",
       "ENTITY                                                                     \n",
       "1.400067e+09                  NaN             4/8/2015            Bronze   \n",
       "1.400078e+09                  NaN           11/21/2016            Bronze   \n",
       "1.400081e+09             4/3/2017             2/8/2013          Platinum   \n",
       "1.400100e+09                  NaN            1/11/2013            Silver   \n",
       "1.400104e+09            4/12/2016            4/12/2016          Platinum   \n",
       "\n",
       "             hra_completion_date vc_completion_date COVERAGE_EFF_DATE ZIP_CD  \\\n",
       "ENTITY                                                                         \n",
       "1.400067e+09                 NaN                NaN         2/23/2015  77064   \n",
       "1.400078e+09                 NaN                NaN          1/1/2014  97405   \n",
       "1.400081e+09       1/1/2018 0:00      3/9/2018 0:00          1/1/2013  32832   \n",
       "1.400100e+09      1/29/2018 0:00     2/13/2018 0:00          1/1/2011  55442   \n",
       "1.400104e+09      12/1/2017 0:00                NaN         11/3/2014  61115   \n",
       "\n",
       "             eventdesc partner_name  CONTACT  \n",
       "ENTITY                                        \n",
       "1.400067e+09       NaN          NaN      NaN  \n",
       "1.400078e+09       NaN          NaN      1.0  \n",
       "1.400081e+09       NaN        APPLE      NaN  \n",
       "1.400100e+09       NaN          NaN      1.0  \n",
       "1.400104e+09       NaN   Google Fit      1.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:,df.columns != 'AGE' ].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must interpret missing values before we can address them. A missing value in the login columns isn't missing data per se; rather, it (normally) indicates that the user hasn't used the specified platform. In my experience, it is safer to treat these as FALSEs or NULLs rather than missing values. As such, I will convert these sorts of columns into booleans.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = df.loc[:, (df.columns != 'AGE') & (df.columns != 'earned_status_adj')].notnull().astype('int')\n",
    "mydata = pd.merge(mydata,df[['AGE','earned_status_adj']],left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = pd.get_dummies(df, columns=['earned_status_adj'], drop_first=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata.loc[:, 'AGE'] = mydata.loc[:, 'AGE'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's make some dummies!\n",
    "\n",
    "mydata = pd.get_dummies(mydata, columns=['AGE'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FIRST_APP_LOGIN_DATE               0\n",
       "FIRST_WEB_LOGIN_DATE               0\n",
       "hra_completion_date                0\n",
       "vc_completion_date                 0\n",
       "COVERAGE_EFF_DATE                  0\n",
       "ZIP_CD                             0\n",
       "eventdesc                          0\n",
       "partner_name                       0\n",
       "CONTACT                        78206\n",
       "earned_status_adj_Bronze      251615\n",
       "earned_status_adj_Gold         83230\n",
       "earned_status_adj_Platinum     68237\n",
       "earned_status_adj_Silver      112869\n",
       "AGE_19-34                     131686\n",
       "AGE_35-49                     205959\n",
       "AGE_50-64                     158120\n",
       "AGE_65-80                      14843\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#I also want to get a sort of \"waterfall\" count...\n",
    "#Basically, this is the count of conv ersions for each feature\n",
    "\n",
    "(mydata == 1).sum(axis=1)#sum of conversions by row (member)\n",
    "(mydata == 1).sum(axis=0)#sum of conversions by column (feature)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's then order our columns appropriately\n",
    "mydata = mydata[mydata.sum().sort_values(ascending = False).index]\n",
    "mydata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finally, our waterfall\n",
    "ax = (mydata == 1).sum(axis=0).plot.bar() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see roughly equivalent eff dates, login dates, and earned statuses, which is not surprising because these are almost defaults. Web login requires some member action, but it's not until we see HRA through eventdesc that we have some feature columns requiring user agency. I therefore think it makes sense to drop coverage eff date and earned status. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata.drop(['COVERAGE_EFF_DATE'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = (mydata == 1).sum(axis=0).plot.bar() #much better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mydata.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata.groupby(mydata.sum(axis=1)).agg(sum) #users grouped by count of feature conversions, up to 7 (and apparently not less than 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = mydata.groupby(mydata.sum(axis=1)).agg(sum).plot.bar(figsize=(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It certainly makes sense for the clustered bars to flatten as we move from left to right. It's interesting that four variables applies to more users than two or three variables, but it makes sense for those easy behaviors to be correlated (whereas vc_completion is harder to achieve). The cohort of users who've contacted us seems quite small, but it's important to note that I am using a pretty small set of data for this initial EDA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see some color!\n",
    "#number of contact conversions for each grouping\n",
    "ax=mydata[mydata.loc[:, 'CONTACT']==1].groupby('CONTACT').agg(sum).plot.bar(figsize=(8,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = mydata.corr()\n",
    "f, ax = plt.subplots(figsize=(10, 8))\n",
    "g = sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n",
    "            square=True, ax=ax)\n",
    "\n",
    "#no single variable seems to correlate too strongly to a contact, but partnername is a promising lead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "linreg = LinearRegression()\n",
    "\n",
    "feature_cols = [i for i in mydata if ('partner' in i)]\n",
    "X = mydata.loc[:, feature_cols]\n",
    "y = mydata.loc[:, 'CONTACT']\n",
    "\n",
    "linreg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = linreg.predict(X)\n",
    "mydata.loc[:, 'y_pred'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(linreg.intercept_)\n",
    "print(linreg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linreg isn't too helpful. Let's try logit. \n",
    "\n",
    "mydata.drop(['y_pred'], axis=1, inplace=True)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "\n",
    "feature_cols = [i for i in mydata if ('CONTACT' not in i and 'pred' not in i)]\n",
    "X = mydata.loc[:, feature_cols]\n",
    "y = mydata.loc[:, 'CONTACT']\n",
    "\n",
    "logreg.fit(X,y)\n",
    "pred = logreg.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.predict_proba(X)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata.loc[:, 'CONTACT_PROB'] = logreg.predict_proba(X)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata.loc[:, 'CONTACT_PROB'].value_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(mydata[mydata.loc[:, 'CONTACT_PROB'] > 0.6]))\n",
    "mydata[mydata.loc[:, 'CONTACT_PROB'] < 0.6\n",
    "      ].head()\n",
    "#presumably the likeliest contacts. \n",
    "#In other words, people for whom all conversions are true have a slightly higher chance of contacting us than \n",
    "#people for whom any other sets of conversions are true. That makes sense--but is it helpful?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This, clearly, still is not very helpful. Let's try a non-parametric approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X = mydata.loc[:, feature_cols]\n",
    "y = mydata.loc[:, 'CONTACT']\n",
    "\n",
    "treeclf = DecisionTreeClassifier(max_depth=3, random_state=1)\n",
    "treeclf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'feature':feature_cols, 'importance':treeclf.feature_importances_})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some comments for next time: \n",
    "I wonder if engagement factors will suffice. I am having some difficulty gathering full demographic date (age, gender, etc.), but will try to blend those factors into the final product. \n",
    "I also have not fully reckoned with a really severe limitation to this data: we don't know whether these engagement factors were true before the user actually contacted us. It could be that they are more engaged post-conversion. One might with some justice assume that this cohort was more inclined to engage anyway, but that's still an unchastened assumption. \n",
    "Finally, 46% contact probability with logistic regression is still less than my goal of >50%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
